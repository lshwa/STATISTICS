# 통계학 7주차 정규과제

📌통계학 정규과제는 매주 정해진 분량의 『*데이터 분석가가 반드시 알아야 할 모든 것*』 을 읽고 학습하는 것입니다. 이번 주는 아래의 **Statistics_7th_TIL**에 나열된 분량을 읽고 `학습 목표`에 맞게 공부하시면 됩니다.

아래의 문제를 풀어보며 학습 내용을 점검하세요. 문제를 해결하는 과정에서 개념을 스스로 정리하고, 필요한 경우 추가자료와 교재를 다시 참고하여 보완하는 것이 좋습니다.

7주차는 `3부. 데이터 분석하기`를 읽고 새롭게 배운 내용을 정리해주시면 됩니다.


## Statistics_7th_TIL

### 3부. 데이터 분석하기
### 13.머신러닝 분석 방법론
### 14.모델 평가



## Study Schedule

| 주차  | 공부 범위     | 완료 여부 |
| ----- | ------------- | --------- |
| 1주차 | 1부 p.2~56    | ✅         |
| 2주차 | 1부 p.57~79   | ✅         |
| 3주차 | 2부 p.82~120  | ✅         |
| 4주차 | 2부 p.121~202 | ✅         |
| 5주차 | 2부 p.203~254 | ✅         |
| 6주차 | 3부 p.300~356 | ✅         |
| 7주차 | 3부 p.357~615 | ✅         |

<!-- 여기까진 그대로 둬 주세요-->

# 13.머신러닝 분석 방법론

```
✅ 학습 목표 :
* 선형 회귀와 다항 회귀를 비교하고, 데이터를 활용하여 적절한 회귀 모델을 구축할 수 있다. 
* 로지스틱 회귀 분석의 개념과 오즈(Odds)의 의미를 설명하고, 분류 문제에 적용할 수 있다.
* k-means 알고리즘의 원리를 설명하고, 적절한 군집 개수를 결정하여 데이터를 군집화할 수 있다.
```

## 13.1. 선형 회귀분석과 Elastic Net(예측모델)
### 13.1.1 회귀분석의 기원과 원리

**회귀분석의 기원**

- 영국의 인류학자 **프랜시스 골턴이 부모의 키와 자식의 키에 대한 유전 법칙을 연구하다가 시작**
  - 사람들의 키는 모집단의 평균으로 회귀한다는 개념



**회귀분석의 원리**

- 각 독립변수의 평균을 통해 종속변수를 예측함. 

- 해당 객체가 소속된 집단의 X( 독립변수) 평균값을 통해 Y (종속변수) 값을 예측하는 것이 기본적인 원리

  - 오차를 최소화하여 그은 선이 바로 회귀 선

- 즉, **종속변수 Y의 값에 영향을 주는 독립변수 X들의 조건을 고려하여 구한 평균값**

- 수식
  $$
  y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \varepsilon
  $$

  - y  : 예측하고자 하는 값
  - 우항 : 각 독립변수가 종속변수에 주는 영향력, 독립변수에 해당하는 값. 
  - 마지막 varepsilon : 모델에 의해 설명되지 않는 구간

<!-- 이미지 1 첨가.-->



**최적의 회귀선을 구하는 방법**

- 모형 적합, 회귀선과 각 관측치를 뜻하는 점간의 거리를 최소화하는 것
- **예측치와 관측치들 간의 수직 거리(오차)의 제곱합을 최소로 하는 직선이 회귀선**
- 최소제곱추정법
- 독립변수가 하나의 회귀분석은 **단순 회귀분석**, 두 개 이상이면 **다중 회귀 분석**이라고 한다. 
  - 다중회귀분석 시에는 다중공선성을 확인할 필요가 있음. 



### 13.1.2 다항 회귀(Polynomial Regression)

독립변수들과 종속변수의 관계가 직선이 아닌 곡선의 관계를 가진 경우 -> 한계 존재

**다항회귀 (Ploynomial Regression)**

- **독립변수와 종속변수의 관계가 곡선형 관계일 때 변수에 각 특성의 제곱을 추가하여 회귀선을 곡선형으로 변환하는 모델**
- 차수가 커질수록 편향은 감소하지만 변동성이 증가하게 된다. 

<!-- 이미지 2 첨가.-->



모델이 완성되면 예측하고자 하는 데이터셋에 모델을 적용하여 예측값을 산출할 수 있다. 

회귀분석의 기본 가설 : 귀무가설이 모든 회귀계수가 0이라는 것 

대립 가설 : 적어도 하나의 변수는 회귀계수가 0이 아니라는 것 

> 다음과 같은 변수별 결과 요약 확인 가능함



<!-- 이미지 3 첨가.-->



- **Parameter Estimate** : 각 변수의 계수 (coefficient)
  - 이 계수의 조합을 통해 회귀식을 만들 수 있다.
- **Standard Error** : 표준오차
  - 값이 크다 : 그만큼 예측값과 실제값의 차이가 크다는 것 
- **T-value** : **노이즈 대비 시그널의 강도**
  - 독립변수와 종속변수 간에 선형관계가 얼마나 강한지를 나타내기 때문에, 값이 커야한다. 
  - 계숫값을 표준오차로 나누어 구할 수 있다. 
- **P-value** : T-value와 관측치 수에 의해 결정되는 값 
- **Tolerance & VIF** : 다중공선성을 판단할 수 있는 값으로 공차한계와 분산팽창지수를 뜻함.
  - 해당 변수의 다른 독립변수들과의 상관관계 수준을 판단하는 기준 
  - 둘은 서로 역수를 취한 값으로 하나의 갑만 보고 판단하기 가능



> 분석가가 수동으로 일일히 변수 조합을 테스트 하는 것은 비효율적
>
> 독립변수가 많을 때 조합을 자동으로 선택할 알고리즘이 필요함. 



**1. 전진 선택법 (Forward Selection)**

- 가장 단순한 변수선택법
- 절편만 있는 모델에서 시작하여 유의미한 독립변수 순으로 변수를 차례로 하나씩 추가하는 방법
- 알고리즘이 단순하기에 빠르다는 장점이 존재하지만, 한 번 선택된 변수는 다시 제거되지 않는다.



**2. 후진 제거법 (Backward Elimination)**

- 독립변수가 포함된 상태에서 시작하여 유의미하지 않는 순으로 설명변수를 하나씩 제거하는 방법
- 모델 적합도가 기준치 이상 감소하는 경우까지 제거하고 종료
  - 한 번 제거된 변수는 다시 추가 X
- 독립변수가 많은 경우 학습 초기에 모든 변숫를 넣고 모델 학습을 하기에 시간이 다소 오래 걸릴 수 있다. 



**3. 단계적 선택법 (Stepwise Selection)**

- 1번과 2번의 장점을 더한 방법
- 처음에는 1번과 같이 변수를 하나씩 추가하기 시작하다가, 선택변 변수가 3개 이상이 되면 변수 추가와 제거를 번갈아 가면서 수행
- 최적의 변수 조합을 찾아내기에 다른 방법보다 최적의 변수 조합을 찾아낼 수 있는 가능성이 높다. 



## 13.2. 로지스틱 회귀분석 (분류모델)
**로지스틱 회귀분석 (Logistic Regression)**

- 앞의 선형회귀분석과 유사하지만 양적이 아닌 질적척도라는 차이 존재
- 특성 수치를 예측하는 것이 아니라 어떤 카테고리에 들어갈지 분류를 하는 모델
- 범주가 3개 이상일 경우에는 다항 로지스틱 회귀분석을 통해 분류 예측 가능



<!-- 이미지 4 첨가.-->



- 0과 1 사이의 S자 곡선의 형태를 갖도록 변환해줘야한다. 
- 로짓 회귀선으로 변환해주기 위해서는 우선 **오즈 값을 구해야한다.**
  - 오즈 : 사건이 발생할 가능성이 발생하지 않을 가능성보다 어느 정도 큰지를 나타내는 값
  - 발생 확률이 1에 가까울 수록 기하급수적으로 커지고 최솟값은 0 => 로그 취하기 



## 13.8. k-means 클러스터링(군집모델)
**k-means**

- KNN은 지도학습, k-means 클러스터링 : 비지도학습
- 미리 가지고 있는 정답 레이블 없이 데이터의 특성과 구조를 발견해 내는 방식 
- 구현 방법이 매우 간단하고 실행 속도가 빠르기 때문에 많이 사용된다. 
- Means : 각 군집의 중심을 듯함
- **중심점과 군집 내 관측치 간의 거리를 비용함수로 하여, 이 함수 값이 최소화되도록 중심점과 군집을 반복적으로 재정의해준다.**

- 단계 설명
  - 1단계 : k 개의 중심점을 임의의 데이터 공간에 선정
  - 2단계 : 각 중심점과 관측치들 간의 유클리드 거리를 계산
  - 3단계 : 각 중심점과 거리가 가까운 관측치들을 해당 군집으로 할당
  - 4단계 : 할당된 군집의 관측치들과 해당 중심점과의 유클리드 거리를 계산
  - 5단계 : 중심점을 군집의 중앙으로 이동 (군집의 관측치들 간 거리 최소 지점)
  - 6단계 : 중심점이 더 이상 이동하지 않을 때 까지 2~5 단계를 반복



# 14. 모델 평가

```
✅ 학습 목표 :
* 유의확률(p-value)을 해석할 때 주의할 점을 설명할 수 있다.
* 분석가가 올바른 주관적 판단을 위한 필수 요소를 식별할 수 있다.
```

## 14.3. 회귀성능 평가지표
**1. R-Square와 Adjusted R-square**

- 결정계수

- 모델의 독립변수들이 종속변수를 설명할 수 있는 설명력을 나타내는 0에서 1 사이의 수치

- 실제값과 예측값의 차이인 오차와, 실제와 실젯값 평균의 차이인 편차와 관련 있다. 

- 수식
  $$
  R^2 = \frac{SS_{\text{reg}}}{SS_{\text{tot}}} = 1 - \frac{SS_{\text{res}}}{SS_{\text{tot}}}
  $$

  - SSR : 회귀식의 추정값과 전체 실젯값 평규노가의 편차 제곱합
  - SSE : 회귀식의 추정값과 실젯값 편차 제곱의 합
  - SST : 실젯값과 전체 실젯값 평균과의 편차 제곱합 

  > SST 값이 작을수록 SSR 값이 클수록, R-square 값이 커지고, 회귀선이 가 데이터를 고르게 설명한다. 



**2. RMSE (Root Mean Square Error)**

- 편차 제곱의 평균에 루트를 씌운 값으로 실제 수치와 예측한 수치와의 차이를 확인하는 전형적인 방법
- 실젯값과 예측값의 표준 편차를 구하는 것 
- 어느정도인지 측정할 수 있어 직관적으로 모델의 정확도를 가늠이 가능하다. 
- 예측값의 스케일에 영향을 받음. 



**3. MAE (Mean Absolute Error)**

- 실제값과 예측값의 차이 절댓값 합을 n으로 나눈 값
- RMSE 는 평균 제곱 오차고, MAE는 평균 절대 오차 
- 제곱한 오차 평균에 제곱근을 해준 것



**4. MAPE (Mean Absolute Percentage Error)**

- 백분율 오차인 MAPE는 MAE를 퍼센트로 변환한 것
- 스케일에 관계없이 절대적인 차이를 비교할 수 있으므로 다른 데이터가 들어간 모델 간 성능을 비교하기에 유용



**5. RMSLE (Root Mean Square Logarithmic Error)**

- 동일한 수식에서 실젯값과 예측값에 1을 더해준 다음 로그에 취해준 평가 방식
- 로그를 취해서 상대적 비율을 비교가 가능함. 
- RMSLE는 RMSF 보다 오차 이상치에 덜 민감. 



**6. AIC 와 BIC**

- 독립변수에 얼마나 많은가에 따른 페널티를 반영하여 계산하는 모델 평가 척도
- AIC 값은 작을 수록 좋은 모델이며 우도가 높을수록, 변수가 적을수록 값은 작아진다. 



## 14.6. 유의확률의 함정
**p 값은 표본의 크기가 커지면 점점 0에 수렴하는 특성을 가진다.**

**따라서 p값이 0.05보다 낮게 나왔다고 항상 유의미한 것은 아니다.**



2016 미국 통계학회의 p값에 대한 성명서 내용

- p 값은 데이터가 통계 모델과 얼마나 호환되지 않는지를 나타낼 수 있다.
- p 값은 연구 가설이 참일 확률 또는 데이터가 우연만으로 생성됐을 확률을 추정하지 않는다.
- 과학적 결론과 사업 또는 정책 결정은 p값이 특정 임곗값을 통과하는지에 기초해서는 안된다.
- 적절한 추론에는 완전한 보고와 투명성이 필요하다.
- p 값 또는 통계적 유의성은 효과의 크기나 결과의 중요성을 측정하지 않는다. 
- p 값은 그 자체로 모델이나 가설에 관한 증거에 대한 훌륭한 척도가 되지 않는다. 



<br>
<br>

# 확인 문제

## **문제 1. 선형 회귀**

> **🧚 칼 피어슨의 아버지와 아들의 키 연구 결과를 바탕으로, 다음 선형 회귀식을 해석하세요.**  
> 칼 피어슨(Karl Pearson)은 아버지(X)와 아들(Y)의 키를 조사한 결과를 바탕으로 아래와 같은 선형 회귀식을 도출하였습니다. 아래의 선형 회귀식을 보고 기울기의 의미를 설명하세요. 
>
> **ŷ = 33.73 + 0.516X**  
>
> - **X**: 아버지의 키 (cm)  
> - **ŷ**: 아들의 예상 키 (cm)  

```
아버지의 키가 1cm 증가할 때, 아들의 예상키는 평균적으로 0.516cm 씩 증가한다는 의미이다. 
둘의 키는 양의 상관관계 입니다. (다만, 1:1로 완전하게 전이되지는 않는다.)
```
---

## **문제 2. 로지스틱 회귀**  

> **🧚 다트비에서는 학생의 학업 성취도를 예측하기 위해 다항 로지스틱 회귀 분석을 수행하였습니다. 학업 성취도(Y)는 ‘낮음’, ‘보통’, ‘높음’ 3가지 범주로 구분되며, 독립 변수는 주당 공부 시간(Study Hours)과 출석률(Attendance Rate)입니다. 단, 기준범주는 '낮음' 입니다.**   

| 변수            | Odds Ratio Estimates | 95% Wald Confidence Limits |
| --------------- | -------------------- | -------------------------- |
| Study Hours     | **2.34**             | (1.89, 2.88)               |
| Attendance Rate | **3.87**             | (2.92, 5.13)               |

> 🔍 Q1. Odds Ratio Estimates(오즈비, OR)의 의미를 해석하세요.

```
오즈비가 2.34라는 것은 주당 공부시간이 1시간 증가할 때 기준 범주인 낮음에 비해 보통이나 높은 학업 성취도를 가질 오즈가 2.34배 증가한다는 의미이다. 
오즈비가 3.87이라는 것은 출석률이 1% 증가할 때
학업 성취도가 보통, 또는 높음인 될 오즈가 3.87배 증가한 다는 것을 뜻한다.
즉, 두 변수 학업 성취도에 긍정적인 영향을 주지만 출석률의 영향이 더 크다는 것을 알 수 있다. 
```



> 🔍 Q2. 95% Wald Confidence Limits의 의미를 설명하세요.
> <!--각 변수의 신뢰구간에 제시된 수치가 의미하는 바를 생각해보세요.-->

```
모집단의 오즈비가 해당 구간 안에 존재할 확률이 95%임을 의미한다. 
또한 이 구간에서 1이 포함되어 있지 않는다는 점은 두 변수 모두 통계적으로 유의미한 영향력이 있음을 나타낸다. 
```



> 🔍 Q3. 이 분석을 기반으로 학업 성취도를 향상시키기 위한 전략을 제안하세요.
> <!--Study Hours와 Attendance Rate 중 어느 변수가 학업 성취도에 더 큰 영향을 미치는지를 고려하여, 학업 성취도를 향상시키기 위한 효과적인 전략을 구체적으로 제시해보세요.-->

```
출석률과 공부시간이 학업 성취도에 큰 영향을 미치지만, 더 높은 영향력을 끼치는 것은 출석률이기 때문에 
출석률 관리를 핵심 전략으로 삼는 것이 중요할 것 같다.
 예를 들면 출석을 잘하는 사람한데 가산점을 주거나, 지각/결석하는 친구들에게는 불이익을 주는 것들이 
 예시로 들 수 있을 것 같다. 
```

---


## **문제 3. k-means 클러스터링**

> **🧚 선교는 고객을 유사한 그룹으로 분류하기 위해 k-means 클러스터링을 적용했습니다. 초기에는 3개의 군집으로 설정했지만, 결과가 만족스럽지 않았습니다. 선교가 최적의 군집 수를 찾기 위해 사용할 수 있는 방법을 한 가지 이상 제시하고 설명하세요.**

```
실루엣 계수를 분석하는 방법이 좋을 것이다.
각 데이터는 포인트의 군집 응집도와 분리도를 비교하여 군집의 품질을 수치화한 지표이기에 -1부터 1사이의 값의 범위를 가지며, 1에 가까울 수록 군집이 잘 분리되어있을 것이다.
여러 k 값에 대해 평균 실루엣 개수를 계산하고 가장 높은 값을 갖는 k 를 선택한다. 
```

### 🎉 수고하셨습니다.



